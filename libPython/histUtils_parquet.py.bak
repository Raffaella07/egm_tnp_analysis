"""
Modern histogram creation utilities supporting Parquet files
Replacement for the Cython histUtils.pyx with Parquet support
"""

import pandas as pd
import numpy as np
import json
import os
import array
from pathlib import Path
import warnings
import uproot
import hist
from typing import Dict, List, Any, Optional, Union

# For backward compatibility with ROOT
try:
    import ROOT as rt
    HAS_ROOT = True
except ImportError:
    HAS_ROOT = False
    warnings.warn("ROOT not available, only Parquet mode supported")


class ParquetHistogramMaker:
    """
    Modern histogram maker that supports both ROOT and Parquet inputs
    Maintains the same interface as the original histUtils.pyx
    """
    
    def __init__(self):
        self.pu_weights = {}
        
    def load_pu_weights(self, json_file: str):
        """Load pileup weights from JSON file"""
        if os.path.exists(json_file):
            with open(json_file, 'r') as f:
                self.pu_weights = json.load(f)
                
    def _evaluate_cut_string(self, df: pd.DataFrame, cut_string: str) -> np.ndarray:
        """
        Evaluate ROOT-style cut string on pandas DataFrame
        Converts ROOT syntax to pandas-compatible expressions
        """
        # Handle common ROOT syntax conversions
        expr = cut_string
        
        # Replace ROOT functions with numpy equivalents
        replacements = {
            'sqrt(': 'np.sqrt(',
            'fabs(': 'np.abs(',
            'abs(': 'np.abs(',
            'cos(': 'np.cos(',
            'sin(': 'np.sin(',
            'tan(': 'np.tan(',
            '&&': ' & ',
            '||': ' | ',
            '!': '~',
        }
        
        for root_func, pandas_func in replacements.items():
            expr = expr.replace(root_func, pandas_func)
            
        # Handle parentheses for boolean operations
        expr = f"({expr})"
        
        try:
            # Evaluate the expression safely
            result = df.eval(expr, engine='python')
            return result.values.astype(bool)
        except Exception as e:
            print(f"Error evaluating cut string: {cut_string}")
            print(f"Converted expression: {expr}")
            print(f"Error: {e}")
            # Return all True as fallback
            return np.ones(len(df), dtype=bool)
    
    def _read_data_parquet(self, sample) -> pd.DataFrame:
        """Read parquet file(s) and return pandas DataFrame"""
        dfs = []
        
        for path in sample.path:
            print(f'Reading parquet file: {path}')
            
            # Handle both single files and directory patterns
            if os.path.isdir(path):
                # Read all parquet files in directory
                parquet_files = list(Path(path).glob("*.parquet"))
                for pf in parquet_files:
                    df_chunk = pd.read_parquet(pf, columns=sample.columns)
                    dfs.append(df_chunk)
            else:
                # Single file
                df_chunk = pd.read_parquet(path, columns=sample.columns)
                dfs.append(df_chunk)
        
        if not dfs:
            raise ValueError(f"No data found for sample {sample.name}")
            
        # Concatenate all chunks
        df = pd.concat(dfs, ignore_index=True)
        print(f'Total events loaded: {len(df)}')
        
        return df
        
    def _read_data_root(self, sample) -> pd.DataFrame:
        """Read ROOT file(s) and return pandas DataFrame"""
        if not HAS_ROOT:
            raise ImportError("ROOT not available for reading ROOT files")
            
        dfs = []
        
        for path in sample.path:
            print(f'Reading ROOT file: {path}')
            
            # Use uproot for reading ROOT files
            with uproot.open(path) as file:
                tree = file[sample.tree]
                
                # Get branches to read
                if sample.columns:
                    branches = sample.columns
                else:
                    # Read all branches
                    branches = tree.keys()
                
                # Convert to pandas DataFrame
                arrays = tree.arrays(branches, library="pd")
                dfs.append(arrays)
        
        if not dfs:
            raise ValueError(f"No data found for sample {sample.name}")
            
        # Concatenate all chunks
        df = pd.concat(dfs, ignore_index=True)
        print(f'Total events loaded: {len(df)}')
        
        return df
    
    def _apply_weights(self, df: pd.DataFrame, sample, cut_mask: np.ndarray) -> np.ndarray:
        """Apply event weights including PU weights if needed"""
        weights = np.ones(len(df))
        
        # Apply cut mask first
        weights = weights * cut_mask.astype(float)
        
        # Apply sample weight if specified
        if sample.weight and sample.weight in df.columns:
            sample_weights = df[sample.weight].values
            
            # Apply max weight cut if specified
            if hasattr(sample, 'maxWeight') and sample.maxWeight < 999:
                sample_weights = np.where(sample_weights < sample.maxWeight, 
                                        sample_weights, 1.0)
            
            weights *= sample_weights
            
        # Apply PU weights if loaded and MC
        if sample.isMC and self.pu_weights and 'nTrueInt' in df.columns:
            pu_bins = np.array(list(self.pu_weights.keys()), dtype=int)
            pu_values = np.array(list(self.pu_weights.values()))
            
            # Map nTrueInt to PU weights
            ntrue_int = df['nTrueInt'].values.astype(int)
            pu_weight_values = np.interp(ntrue_int, pu_bins, pu_values)
            weights *= pu_weight_values
            
        return weights
    
    def makePassFailHistograms(self, sample, flag: str, flag2: str, bindef: Dict, var: Dict):
        """
        Create pass/fail histograms for efficiency measurements
        
        Parameters:
        -----------
        sample : tnpSample object
            Sample configuration
        flag : str
            Primary flag definition (pass condition)
        flag2 : str  
            Secondary flag definition (fallback)
        bindef : dict
            Bin definitions
        var : dict
            Variable definition for histograms
        """
        
        print(f"Creating histograms for sample: {sample.name}")
        print(f"Flag: {flag}")
        print(f"Flag2: {flag2}")
        
        # Determine input format and read data
        input_format = sample.get_format()
        
        if input_format == "parquet":
            df = self._read_data_parquet(sample)
        else:
            df = self._read_data_root(sample)
        
        # Create output directory if needed
        os.makedirs(os.path.dirname(sample.histFile), exist_ok=True)
        
        # Prepare histograms using hist library for Python
        pass_hists = {}
        fail_hists = {}
        
        # Setup variable binning
        if var['name'] not in df.columns:
            raise ValueError(f"Variable {var['name']} not found in data")
            
        hist_axis = hist.axis.Regular(var['nbins'], var['min'], var['max'], 
                                    name=var['name'], label=var['name'])
        
        # Process each kinematic bin
        for i, bin_def in enumerate(bindef['bins']):
            bin_name = bin_def['name']
            bin_title = bin_def['title']
            
            print(f"Processing bin {i}: {bin_name}")
            
            # Create histograms for this bin
            pass_hists[bin_name] = hist.Hist(hist_axis, name=f"{bin_name}_Pass")
            fail_hists[bin_name] = hist.Hist(hist_axis, name=f"{bin_name}_Fail")
            
            # Apply bin cuts
            bin_cut = bin_def['cut']
            
            # Add MC truth requirement if needed
            if sample.mcTruth:
                bin_cut = f"{bin_cut} && mcTrue==1"
                
            # Add sample-specific cuts
            if sample.cut:
                bin_cut = f"{bin_cut} && {sample.cut}"
            
            # Evaluate cut string
            cut_mask = self._evaluate_cut_string(df, bin_cut)
            
            # Apply weights
            weights = self._apply_weights(df, sample, cut_mask)
            
            # Get data for events passing cuts
            selected_df = df[cut_mask]
            selected_weights = weights[cut_mask]
            
            if len(selected_df) == 0:
                print(f"Warning: No events pass cuts for bin {bin_name}")
                continue
                
            # Evaluate flag conditions
            try:
                # Try primary flag first
                flag_result = self._evaluate_cut_string(selected_df, flag)
            except:
                try:
                    # Fall back to secondary flag
                    print(f"Primary flag failed, trying secondary flag for bin {bin_name}")
                    flag_result = self._evaluate_cut_string(selected_df, flag2)
                except:
                    print(f"Both flags failed for bin {bin_name}, skipping")
                    continue
            
            # Fill histograms
            var_values = selected_df[var['name']].values
            
            # Pass events
            pass_mask = flag_result
            if np.any(pass_mask):
                pass_hists[bin_name].fill(
                    var_values[pass_mask],
                    weight=selected_weights[pass_mask]
                )
            
            # Fail events  
            fail_mask = ~flag_result
            if np.any(fail_mask):
                fail_hists[bin_name].fill(
                    var_values[fail_mask], 
                    weight=selected_weights[fail_mask]
                )
            
            # Print efficiency info
            n_pass = np.sum(selected_weights[pass_mask]) if np.any(pass_mask) else 0
            n_fail = np.sum(selected_weights[fail_mask]) if np.any(fail_mask) else 0
            n_total = n_pass + n_fail
            
            if n_total > 0:
                eff = n_pass / n_total
                eff_err = np.sqrt(n_pass * n_fail / n_total) / n_total
                print(f"  Bin {bin_name}: pass={n_pass:.1f}, fail={n_fail:.1f}, eff={eff:.3f}±{eff_err:.3f}")
        
        # Save histograms to file
        self._save_histograms(sample.histFile, pass_hists, fail_hists)
        
        print(f"Histograms saved to: {sample.histFile}")
    
    def _save_histograms(self, filename: str, pass_hists: Dict, fail_hists: Dict):
        """Save histograms to file - prioritize ROOT format for workflow compatibility"""
        
        # Priority 1: Save as ROOT file if possible (for workflow compatibility)
        root_saved = False
        if HAS_ROOT and filename.endswith('.root'):
            try:
                self._save_root_histograms(filename, pass_hists, fail_hists)
                print(f"Histograms saved as ROOT: {filename}")
                root_saved = True
            except Exception as e:
                print(f"ROOT save failed: {e}, falling back to Python formats")
        
        # Priority 2: Also save as pickle and numpy for pure Python access
        if not root_saved or not filename.endswith('.root'):
            import pickle
            hist_data = {
                'pass': pass_hists,
                'fail': fail_hists,
                'metadata': {
                    'format': 'hist_library',
                    'bins': len(pass_hists),
                    'created': pd.Timestamp.now().isoformat()
                }
            }
            
            # Save as pickle file
            pickle_file = filename.replace('.root', '.pkl')
            with open(pickle_file, 'wb') as f:
                pickle.dump(hist_data, f)
            
            # Also save as numpy arrays for compatibility
            numpy_file = filename.replace('.root', '.npz')
            numpy_data = {}
            
            for bin_name in pass_hists.keys():
                pass_h = pass_hists[bin_name]
                fail_h = fail_hists[bin_name]
                
                numpy_data[f'{bin_name}_Pass_values'] = pass_h.values()
                numpy_data[f'{bin_name}_Pass_edges'] = pass_h.axes[0].edges
                numpy_data[f'{bin_name}_Pass_variances'] = pass_h.variances()
                
                numpy_data[f'{bin_name}_Fail_values'] = fail_h.values()
                numpy_data[f'{bin_name}_Fail_edges'] = fail_h.axes[0].edges  
                numpy_data[f'{bin_name}_Fail_variances'] = fail_h.variances()
            
            np.savez_compressed(numpy_file, **numpy_data)
            
            print(f"Histograms also saved as:")
            print(f"  Pickle: {pickle_file}")
            print(f"  NumPy: {numpy_file}")
            
            if not root_saved:
                print("WARNING: ROOT format not available, downstream fitting may require ROOT histograms")
    
    def _save_root_histograms(self, filename: str, pass_hists: Dict, fail_hists: Dict):
        """Save histograms in ROOT format (optional, for backward compatibility)"""
        
        print(f"Saving {len(pass_hists)} bins to ROOT file: {filename}")
        
        root_file = rt.TFile(filename, "RECREATE")
        
        for bin_name in pass_hists.keys():
            # Convert hist objects to ROOT histograms
            pass_h = pass_hists[bin_name]
            fail_h = fail_hists[bin_name]
            
            # Get histogram properties
            edges = pass_h.axes[0].edges
            n_bins = len(edges) - 1
            
            # Create histogram names
            pass_name = f"{bin_name}_Pass"
            fail_name = f"{bin_name}_Fail"
            
            # Create ROOT histograms with proper binning
            bin_widths = np.diff(edges)
            is_uniform = np.allclose(bin_widths, bin_widths[0], rtol=1e-6)
            
            if is_uniform:
                # Uniform binning
                xmin, xmax = float(edges[0]), float(edges[-1])
                pass_root = rt.TH1D(pass_name, pass_name, n_bins, xmin, xmax)
                fail_root = rt.TH1D(fail_name, fail_name, n_bins, xmin, xmax)
            else:
                # Variable binning
                edges_array = array.array('d', edges)
                pass_root = rt.TH1D(pass_name, pass_name, n_bins, edges_array)
                fail_root = rt.TH1D(fail_name, fail_name, n_bins, edges_array)
            
            # Fill ROOT histograms with content and errors
            pass_values = pass_h.values()
            pass_variances = pass_h.variances()
            fail_values = fail_h.values()
            fail_variances = fail_h.variances()
            
            for i in range(n_bins):
                # ROOT bins are 1-indexed
                pass_root.SetBinContent(i+1, float(pass_values[i]))
                pass_root.SetBinError(i+1, float(np.sqrt(pass_variances[i])))
                
                fail_root.SetBinContent(i+1, float(fail_values[i]))
                fail_root.SetBinError(i+1, float(np.sqrt(fail_variances[i])))
            
            # Set proper statistics
            pass_root.SetEntries(int(np.sum(pass_values)))
            fail_root.SetEntries(int(np.sum(fail_values)))
            
            # Write to file
            root_file.cd()
            pass_root.Write(pass_name, rt.TObject.kOverwrite)
            fail_root.Write(fail_name, rt.TObject.kOverwrite)
        
        root_file.Close()
        print(f"✓ ROOT histograms saved successfully ({os.path.getsize(filename)} bytes)")
        

# Global instance for backward compatibility
histogram_maker = ParquetHistogramMaker()


def makePassFailHistograms(sample, flag, flag2, bindef, var):
    """
    Backward compatibility function
    """
    return histogram_maker.makePassFailHistograms(sample, flag, flag2, bindef, var)


def load_pu_weights(json_file: str):
    """Load PU weights from JSON"""
    return histogram_maker.load_pu_weights(json_file)
